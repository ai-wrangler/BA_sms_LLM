{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ai-wrangler/BA_sms_LLM/blob/main/SMS_LLM_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b408d35",
      "metadata": {
        "id": "5b408d35"
      },
      "source": [
        "# SMS Spam Classification with Embeddings and Gemini LLM\n",
        "This Colab-ready notebook recreates the Lab 5 text mining workflow from Weka using Python pipelines and an LLM available to Colab Pro users. You'll load the original ARFF dataset, build embedding-based classifiers, invoke Gemini for zero-shot spam detection, and compare the evaluation metrics across approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563fdcad",
      "metadata": {
        "id": "563fdcad"
      },
      "source": [
        "## How to use this notebook in Google Colab Pro\n",
        "1. Upload `SMS_LLM_Colab.ipynb` to Colab (File → Upload notebook) or open it from Drive.\n",
        "2. Runtime → Change runtime type → make sure Python 3.10+; GPU is optional.\n",
        "3. Prepare the dataset: copy `TextCollection_sms.arff` to your Drive or download it locally so you can upload it when prompted.\n",
        "4. Store your Gemini API key securely (`Tools → Secrets` in Colab or `google.colab.userdata`). This notebook expects an environment variable called `GOOGLE_API_KEY`.\n",
        "5. Run the cells in order—each is annotated to match the lab workflow and highlight differences between embeddings and LLM-based classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bfa7d50"
      },
      "source": [
        "### Fixing 'Invalid Notebook' Error on GitHub\n",
        "\n",
        "To resolve the 'state' key missing error when rendering your notebook on GitHub, you should clear all cell outputs before saving and committing your notebook. Here's how to do it in Google Colab:\n",
        "\n",
        "1.  **Open your notebook** in Google Colab.\n",
        "2.  Go to the **'Runtime'** menu at the top.\n",
        "3.  Select **'Clear all outputs'**.\n",
        "4.  **Save the notebook** (File > Save).\n",
        "5.  Then, you can **download the `.ipynb` file** and upload it to GitHub, or sync it if you are using Google Drive integration with GitHub."
      ],
      "id": "0bfa7d50"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75b5c66d",
      "metadata": {
        "id": "75b5c66d"
      },
      "outputs": [],
      "source": [
        "# Install libraries that are not included in the base Colab runtime\n",
        "%pip install -q pandas numpy scikit-learn seaborn matplotlib sentence-transformers google-generativeai scipy liac-arff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6555bf0a",
      "metadata": {
        "id": "6555bf0a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from google.colab import ai\n",
        "import arff # Replaced from scipy.io import arff with import arff (for liac-arff)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    f1_score, precision_score, recall_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef6109a1",
      "metadata": {
        "id": "ef6109a1"
      },
      "outputs": [],
      "source": [
        "# Reproducibility helpers\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "random.seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "759efd82",
      "metadata": {
        "id": "759efd82"
      },
      "source": [
        "## Load the SMS Spam ARFF dataset\n",
        "The lab uses `TextCollection_sms.arff`. Use one of the cells below to make it available in the Colab filesystem. Uploading via the UI is the quickest path if the file is on your laptop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8d7edba",
      "metadata": {
        "id": "d8d7edba"
      },
      "outputs": [],
      "source": [
        "# Option A: Mount Google Drive (run this if the ARFF lives in Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# After mounting, set ARFF_PATH = '/content/drive/MyDrive/path/to/TextCollection_sms.arff'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f834df1c",
      "metadata": {
        "id": "f834df1c"
      },
      "outputs": [],
      "source": [
        "# Option B: Upload the ARFF manually (run this if the file is on your machine)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "ARFF_PATH = next(iter(uploaded))  # use the first uploaded filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65925520",
      "metadata": {
        "id": "65925520"
      },
      "outputs": [],
      "source": [
        "# If you mounted Drive instead of uploading, set the explicit path here.\n",
        "# Example: ARFF_PATH = '/content/drive/MyDrive/datasets/TextCollection_sms.arff'\n",
        "ARFF_PATH = locals().get('ARFF_PATH', 'TextCollection_sms.arff')\n",
        "print(f'Using dataset located at: {ARFF_PATH}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2322496",
      "metadata": {
        "id": "f2322496"
      },
      "outputs": [],
      "source": [
        "# Read the ARFF file into a DataFrame and mirror the original lab schema\n",
        "# Using liac-arff as scipy.io.arff does not support string attributes\n",
        "arff_data = arff.load(open(ARFF_PATH, 'r'))\n",
        "raw_data = arff_data['data']\n",
        "attributes = arff_data['attributes']\n",
        "column_names = [attr[0] for attr in attributes]\n",
        "\n",
        "sms_df = pd.DataFrame(raw_data, columns=column_names)\n",
        "# liac-arff reads strings directly, so no decoding is needed\n",
        "sms_df = sms_df.rename(columns={'Text': 'message', 'class-att': 'label'})\n",
        "sms_df['label'] = sms_df['label'].map({'0': 'ham', '1': 'spam'})\n",
        "sms_df['char_len'] = sms_df['message'].str.len()\n",
        "sms_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6b001f",
      "metadata": {
        "id": "ae6b001f"
      },
      "outputs": [],
      "source": [
        "# Quick class balance check\n",
        "ax = sms_df['label'].value_counts().sort_index().plot(kind='bar', color=['#4C72B0', '#DD8452'])\n",
        "ax.set(title='Class distribution', xlabel='Label', ylabel='Count')\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2, p.get_height()),\n",
        "                ha='center', va='bottom')\n",
        "plt.show()\n",
        "sms_df.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b37986",
      "metadata": {
        "id": "01b37986"
      },
      "outputs": [],
      "source": [
        "# Train/test split mirroring the lab evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    sms_df['message'],\n",
        "    sms_df['label'],\n",
        "    test_size=0.2,\n",
        "    stratify=sms_df['label'],\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "print(f'Train set: {X_train.shape[0]} messages | Test set: {X_test.shape[0]} messages')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c28f02b5",
      "metadata": {
        "id": "c28f02b5"
      },
      "outputs": [],
      "source": [
        "# Shared evaluation helpers for classical models and the LLM\n",
        "results = []\n",
        "\n",
        "def capture_metrics(name: str, y_true, y_pred) -> pd.Series:\n",
        "    metrics = {\n",
        "        'model': name,\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred, pos_label='spam'),\n",
        "        'recall': recall_score(y_true, y_pred, pos_label='spam'),\n",
        "        'f1': f1_score(y_true, y_pred, pos_label='spam')\n",
        "    }\n",
        "    results.append(metrics)\n",
        "    print(json.dumps(metrics, indent=2))\n",
        "    return pd.Series(metrics)\n",
        "\n",
        "def plot_confusion(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=['ham', 'spam'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['ham', 'spam'], yticklabels=['ham', 'spam'])\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d029f5a",
      "metadata": {
        "id": "9d029f5a"
      },
      "source": [
        "## Baseline 1: TF–IDF + Logistic Regression\n",
        "Replicates the bag-of-words style features typically explored in Weka's text mining lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2955b8",
      "metadata": {
        "id": "2c2955b8"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(lowercase=True, stop_words='english', min_df=3, ngram_range=(1, 2))\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "bow_clf = LogisticRegression(max_iter=200, random_state=RANDOM_STATE, n_jobs=None)\n",
        "bow_clf.fit(X_train_tfidf, y_train)\n",
        "bow_preds = bow_clf.predict(X_test_tfidf)\n",
        "capture_metrics('TFIDF + LogisticRegression', y_test, bow_preds)\n",
        "print(classification_report(y_test, bow_preds))\n",
        "plot_confusion(y_test, bow_preds, 'TF-IDF Logistic Regression Confusion Matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f11a420e",
      "metadata": {
        "id": "f11a420e"
      },
      "source": [
        "## Baseline 2: SentenceTransformer Embeddings + Logistic Regression\n",
        "Uses a semantic embedding (MiniLM) to capture contextual similarity beyond word frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c02b33b",
      "metadata": {
        "id": "7c02b33b"
      },
      "outputs": [],
      "source": [
        "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "X_train_emb = embedder.encode(X_train.tolist(), show_progress_bar=True, batch_size=128)\n",
        "X_test_emb = embedder.encode(X_test.tolist(), show_progress_bar=True, batch_size=128)\n",
        "\n",
        "embed_clf = LogisticRegression(max_iter=500, random_state=RANDOM_STATE)\n",
        "embed_clf.fit(X_train_emb, y_train)\n",
        "embed_preds = embed_clf.predict(X_test_emb)\n",
        "capture_metrics('MiniLM Embeddings + LogisticRegression', y_test, embed_preds)\n",
        "print(classification_report(y_test, embed_preds))\n",
        "plot_confusion(y_test, embed_preds, 'MiniLM Logistic Regression Confusion Matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7603df",
      "metadata": {
        "id": "6c7603df"
      },
      "source": [
        "## Configure Gemini for LLM-based Zero/Few-shot Classification\n",
        "You need an active Gemini API key. In Colab Pro you can store it via `Tools → Secrets` and retrieve it with `google.colab.userdata.get('GOOGLE_API_KEY')`. Alternatively, set `os.environ['GOOGLE_API_KEY']` manually (just avoid hard-coding secrets in plain text)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2d92beb",
      "metadata": {
        "id": "c2d92beb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "GOOGLE_API_KEY = os.environ.get('GEMINI_API_KEY')\n",
        "if GOOGLE_API_KEY is None:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError('Missing Gemini API key. Set GOOGLE_API_KEY via Colab secrets or environment variables before continuing.')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "GEMINI_MODEL = 'gemini-2.0-flash-lite'\n",
        "llm = genai.GenerativeModel(GEMINI_MODEL)\n",
        "print(f'Gemini model ready via google.generativeai: {GEMINI_MODEL}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "348a2d2a"
      },
      "source": [
        "LLM_SAMPLE_SIZE = 200\n",
        "llm_eval_df = (\n",
        "    pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
        "    .rename(columns={'message': 'text', 'label': 'label'})\n",
        ")\n",
        "if LLM_SAMPLE_SIZE and LLM_SAMPLE_SIZE < len(llm_eval_df):\n",
        "    llm_eval_df = (\n",
        "        llm_eval_df\n",
        "        .groupby('label', group_keys=False)\n",
        "        .apply(\n",
        "            lambda grp: grp.sample(\n",
        "                n=max(1, int(LLM_SAMPLE_SIZE * len(grp) / len(llm_eval_df))),\n",
        "                random_state=RANDOM_STATE\n",
        "            )\n",
        "        )\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are a strict SMS spam filter. Respond with a single word: \"\n",
        "    '\"Spam\" for unsolicited or fraudulent messages, \"Ham\" for regular '\n",
        "    'personal/business messages. Never explain your answer.'\n",
        ")\n",
        "\n",
        "def classify_with_gemini(text: str, retry: int = 3, backoff: float = 2.0) -> str:\n",
        "    user_prompt = f'Message: \"{text}\"\\nLabel:'\n",
        "    for attempt in range(retry):\n",
        "        try:\n",
        "            response = llm.generate_content([system_prompt, user_prompt])\n",
        "            raw = response.text.strip().lower()\n",
        "            if 'spam' in raw and 'ham' in raw:\n",
        "                raw = raw.split()[0]\n",
        "            if 'spam' in raw:\n",
        "                return 'spam'\n",
        "            if 'ham' in raw:\n",
        "                return 'ham'\n",
        "        except Exception as error:\n",
        "            if attempt == retry - 1:\n",
        "                print(f'LLM classification failed after retries: {error}')\n",
        "                return 'ham'\n",
        "            time.sleep(backoff * (attempt + 1))\n",
        "    return 'ham'\n",
        "\n",
        "llm_predictions = []\n",
        "for idx, row in llm_eval_df.iterrows():\n",
        "    prediction = classify_with_gemini(row['text'])\n",
        "    llm_predictions.append(prediction)\n",
        "    if (idx + 1) % 25 == 0 or idx + 1 == len(llm_eval_df):\n",
        "        print(f\"Processed {idx + 1}/{len(llm_eval_df)} messages\")\n",
        "\n",
        "llm_eval_df['prediction'] = llm_predictions\n",
        "\n",
        "capture_metrics('gemini-2.0-flash-lite (LLM few-shot)', llm_eval_df['label'], llm_eval_df['prediction'])\n",
        "print(classification_report(llm_eval_df['label'], llm_eval_df['prediction']))\n",
        "plot_confusion(llm_eval_df['label'], llm_eval_df['prediction'], 'Gemini Confusion Matrix (Sample)')"
      ],
      "id": "348a2d2a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "07e7af2d",
      "metadata": {
        "id": "07e7af2d"
      },
      "source": [
        "## LLM Inference Loop\n",
        "Gemini calls incur cost/latency, so we evaluate on a stratified subset of the held-out test set (default 200 messages). Adjust `LLM_SAMPLE_SIZE` for deeper comparisons if desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1efc3b67",
      "metadata": {
        "id": "1efc3b67"
      },
      "outputs": [],
      "source": [
        "LLM_SAMPLE_SIZE = 200\n",
        "llm_eval_df = (\n",
        "    pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
        "    .rename(columns={'message': 'text', 'label': 'label'})\n",
        ")\n",
        "if LLM_SAMPLE_SIZE and LLM_SAMPLE_SIZE < len(llm_eval_df):\n",
        "    llm_eval_df = (\n",
        "        llm_eval_df\n",
        "        .groupby('label', group_keys=False)\n",
        "        .apply(\n",
        "            lambda grp: grp.sample(\n",
        "                n=max(1, int(LLM_SAMPLE_SIZE * len(grp) / len(llm_eval_df))),\n",
        "                random_state=RANDOM_STATE\n",
        "            )\n",
        "        )\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are a strict SMS spam filter. Respond with a single word: \"\n",
        "    '\"Spam\" for unsolicited or fraudulent messages, \"Ham\" for regular '\n",
        "    'personal/business messages. Never explain your answer.'\n",
        ")\n",
        "\n",
        "def classify_with_gemini(text: str, retry: int = 3, backoff: float = 2.0) -> str:\n",
        "    user_prompt = f'Message: \"{text}\"\\nLabel:'\n",
        "    for attempt in range(retry):\n",
        "        try:\n",
        "            response = llm.generate_content([system_prompt, user_prompt])\n",
        "            raw = response.text.strip().lower()\n",
        "            if 'spam' in raw and 'ham' in raw:\n",
        "                raw = raw.split()[0]\n",
        "            if 'spam' in raw:\n",
        "                return 'spam'\n",
        "            if 'ham' in raw:\n",
        "                return 'ham'\n",
        "        except Exception as error:\n",
        "            if attempt == retry - 1:\n",
        "                print(f'LLM classification failed after retries: {error}')\n",
        "                return 'ham'\n",
        "            time.sleep(backoff * (attempt + 1))\n",
        "    return 'ham'\n",
        "\n",
        "llm_predictions = []\n",
        "for idx, row in llm_eval_df.iterrows():\n",
        "    prediction = classify_with_gemini(row['text'])\n",
        "    llm_predictions.append(prediction)\n",
        "    if (idx + 1) % 25 == 0 or idx + 1 == len(llm_eval_df):\n",
        "        print(f\"Processed {idx + 1}/{len(llm_eval_df)} messages\")\n",
        "\n",
        "llm_eval_df['prediction'] = llm_predictions\n",
        "\n",
        "capture_metrics('Gemini 1.5 Pro (LLM few-shot)', llm_eval_df['label'], llm_eval_df['prediction'])\n",
        "print(classification_report(llm_eval_df['label'], llm_eval_df['prediction']))\n",
        "plot_confusion(llm_eval_df['label'], llm_eval_df['prediction'], 'Gemini Confusion Matrix (Sample)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a43b49c",
      "metadata": {
        "id": "0a43b49c"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "results_df.sort_values('f1', ascending=False).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b1cff4a",
      "metadata": {
        "id": "9b1cff4a"
      },
      "source": [
        "### Observations\n",
        "* **TF–IDF + Logistic Regression** mirrors the original Weka text-mining pipeline and usually delivers high recall on overt spam phrases such as \"free entry\" or \"claim now\".\n",
        "* **MiniLM embeddings** capture semantics and can reduce false positives on nuanced ham, at the cost of downloading the encoder and adding encoding latency.\n",
        "* **Gemini 1.5 Pro** needs no training data but depends on prompt wording and incurs per-token cost; it shines on context-heavy alerts yet may miss terse slang spam without examples.\n",
        "* Hybrid scoring (e.g., fall back to Gemini when the classical models disagree) is a strong extension for future lab work."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c0cfda",
      "metadata": {
        "id": "57c0cfda"
      },
      "source": [
        "## Optional: Export Artifacts\n",
        "If you want to retain the evaluation outputs in Drive, run the cell below and then use the Colab file browser or `drive.mount` to move the CSVs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "668b55b9",
      "metadata": {
        "id": "668b55b9"
      },
      "outputs": [],
      "source": [
        "results_df.to_csv('spam_lab_results_summary.csv', index=False)\n",
        "llm_eval_df.to_csv('spam_lab_llm_predictions.csv', index=False)\n",
        "print('Artifacts saved locally. Upload to Drive if you need persistent storage.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e098625",
      "metadata": {
        "id": "2e098625"
      },
      "source": [
        "## Next Steps\n",
        "1. Swap Gemini for a lower-latency model such as `gemini-1.5-flash` to quantify cost/quality trade-offs.\n",
        "2. Prompt-tune the LLM with a few labelled examples when you need higher recall on shorthand or code-mixed spam.\n",
        "3. Experiment with alternative embedding models (`all-mpnet-base-v2`, fastText) and blend their scores with the LLM for ensemble voting.\n",
        "4. Track execution time and API spend so you can justify when the LLM adjudicator adds value beyond classical baselines."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}