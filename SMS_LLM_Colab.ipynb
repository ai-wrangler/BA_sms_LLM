{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ai-wrangler/BA_sms_LLM/blob/main/SMS_LLM_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b408d35",
      "metadata": {
        "id": "5b408d35"
      },
      "source": [
        "# SMS Spam Classification with Embeddings and HuggingFace LLM\n",
        "This Colab-ready notebook recreates the Lab 5 text mining workflow from Weka using Python pipelines and HuggingFace's free Inference API. You'll load the original ARFF dataset, build embedding-based classifiers, invoke HuggingFace models for zero-shot spam detection, and compare the evaluation metrics across approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563fdcad",
      "metadata": {
        "id": "563fdcad"
      },
      "source": [
        "## How to use this notebook in Google Colab\n",
        "1. Upload `SMS_LLM_Colab.ipynb` to Colab (File → Upload notebook) or open it from Drive.\n",
        "2. Runtime → Change runtime type → make sure Python 3.10+; GPU is optional.\n",
        "3. Prepare the dataset: copy `TextCollection_sms.arff` to your Drive or download it locally so you can upload it when prompted.\n",
        "4. Get a free HuggingFace API token from https://huggingface.co/settings/tokens (create a \"Read\" token). Store it securely (`Tools → Secrets` in Colab or `google.colab.userdata`). This notebook expects an environment variable called `HF_API_KEY`.\n",
        "5. Run the cells in order—each is annotated to match the lab workflow and highlight differences between embeddings and LLM-based classification.\n",
        "\n",
        "**HuggingFace Free Tier Limits:**\n",
        "- Rate limit: 1,000 requests/day for free tier (varies by model)\n",
        "- Most models support 1,024-2,048 tokens per request\n",
        "- Some popular models may have lower rate limits during peak usage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bfa7d50",
      "metadata": {
        "id": "0bfa7d50"
      },
      "source": [
        "### Fixing 'Invalid Notebook' Error on GitHub\n",
        "\n",
        "To resolve the 'state' key missing error when rendering your notebook on GitHub, you should clear all cell outputs before saving and committing your notebook. Here's how to do it in Google Colab:\n",
        "\n",
        "1.  **Open your notebook** in Google Colab.\n",
        "2.  Go to the **'Runtime'** menu at the top.\n",
        "3.  Select **'Clear all outputs'**.\n",
        "4.  **Save the notebook** (File > Save).\n",
        "5.  Then, you can **download the `.ipynb` file** and upload it to GitHub, or sync it if you are using Google Drive integration with GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "75b5c66d",
      "metadata": {
        "id": "75b5c66d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install libraries that are not included in the base Colab runtime\n",
        "%pip install -q pandas numpy scikit-learn seaborn matplotlib sentence-transformers scipy liac-arff huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6555bf0a",
      "metadata": {
        "id": "6555bf0a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "import arff # Replaced from scipy.io import arff with import arff (for liac-arff)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    f1_score, precision_score, recall_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef6109a1",
      "metadata": {
        "id": "ef6109a1"
      },
      "outputs": [],
      "source": [
        "# Reproducibility helpers\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "random.seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "759efd82",
      "metadata": {
        "id": "759efd82"
      },
      "source": [
        "## Load the SMS Spam ARFF dataset\n",
        "The lab uses `TextCollection_sms.arff`. Use one of the cells below to make it available in the Colab filesystem. Uploading via the UI is the quickest path if the file is on your laptop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8d7edba",
      "metadata": {
        "id": "d8d7edba"
      },
      "outputs": [],
      "source": [
        "# Option A: Mount Google Drive (run this if the ARFF lives in Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# After mounting, set ARFF_PATH = '/content/drive/MyDrive/path/to/TextCollection_sms.arff'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f834df1c",
      "metadata": {
        "id": "f834df1c"
      },
      "outputs": [],
      "source": [
        "# Option B: Upload the ARFF manually (run this if the file is on your machine)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "ARFF_PATH = next(iter(uploaded))  # use the first uploaded filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65925520",
      "metadata": {
        "id": "65925520"
      },
      "outputs": [],
      "source": [
        "# If you mounted Drive instead of uploading, set the explicit path here.\n",
        "# Example: ARFF_PATH = '/content/drive/MyDrive/datasets/TextCollection_sms.arff'\n",
        "ARFF_PATH = locals().get('ARFF_PATH', 'TextCollection_sms.arff')\n",
        "print(f'Using dataset located at: {ARFF_PATH}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2322496",
      "metadata": {
        "id": "f2322496"
      },
      "outputs": [],
      "source": [
        "# Read the ARFF file into a DataFrame and mirror the original lab schema\n",
        "# Using liac-arff as scipy.io.arff does not support string attributes\n",
        "arff_data = arff.load(open(ARFF_PATH, 'r'))\n",
        "raw_data = arff_data['data']\n",
        "attributes = arff_data['attributes']\n",
        "column_names = [attr[0] for attr in attributes]\n",
        "\n",
        "sms_df = pd.DataFrame(raw_data, columns=column_names)\n",
        "# liac-arff reads strings directly, so no decoding is needed\n",
        "sms_df = sms_df.rename(columns={'Text': 'message', 'class-att': 'label'})\n",
        "sms_df['label'] = sms_df['label'].map({'0': 'ham', '1': 'spam'})\n",
        "sms_df['char_len'] = sms_df['message'].str.len()\n",
        "sms_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6b001f",
      "metadata": {
        "id": "ae6b001f"
      },
      "outputs": [],
      "source": [
        "# Quick class balance check\n",
        "ax = sms_df['label'].value_counts().sort_index().plot(kind='bar', color=['#4C72B0', '#DD8452'])\n",
        "ax.set(title='Class distribution', xlabel='Label', ylabel='Count')\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2, p.get_height()),\n",
        "                ha='center', va='bottom')\n",
        "plt.show()\n",
        "sms_df.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b37986",
      "metadata": {
        "id": "01b37986"
      },
      "outputs": [],
      "source": [
        "# Train/test split mirroring the lab evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    sms_df['message'],\n",
        "    sms_df['label'],\n",
        "    test_size=0.2,\n",
        "    stratify=sms_df['label'],\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "print(f'Train set: {X_train.shape[0]} messages | Test set: {X_test.shape[0]} messages')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c28f02b5",
      "metadata": {
        "id": "c28f02b5"
      },
      "outputs": [],
      "source": [
        "# Shared evaluation helpers for classical models and the LLM\n",
        "results = []\n",
        "\n",
        "def capture_metrics(name: str, y_true, y_pred) -> pd.Series:\n",
        "    metrics = {\n",
        "        'model': name,\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred, pos_label='spam'),\n",
        "        'recall': recall_score(y_true, y_pred, pos_label='spam'),\n",
        "        'f1': f1_score(y_true, y_pred, pos_label='spam')\n",
        "    }\n",
        "    results.append(metrics)\n",
        "    print(json.dumps(metrics, indent=2))\n",
        "    return pd.Series(metrics)\n",
        "\n",
        "def plot_confusion(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=['ham', 'spam'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['ham', 'spam'], yticklabels=['ham', 'spam'])\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d029f5a",
      "metadata": {
        "id": "9d029f5a"
      },
      "source": [
        "## Baseline 1: TF–IDF + Logistic Regression\n",
        "Replicates the bag-of-words style features typically explored in Weka's text mining lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2955b8",
      "metadata": {
        "id": "2c2955b8"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(lowercase=True, stop_words='english', min_df=3, ngram_range=(1, 2))\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "bow_clf = LogisticRegression(max_iter=200, random_state=RANDOM_STATE, n_jobs=None)\n",
        "bow_clf.fit(X_train_tfidf, y_train)\n",
        "bow_preds = bow_clf.predict(X_test_tfidf)\n",
        "capture_metrics('TFIDF + LogisticRegression', y_test, bow_preds)\n",
        "print(classification_report(y_test, bow_preds))\n",
        "plot_confusion(y_test, bow_preds, 'TF-IDF Logistic Regression Confusion Matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f11a420e",
      "metadata": {
        "id": "f11a420e"
      },
      "source": [
        "## Baseline 2: SentenceTransformer Embeddings + Logistic Regression\n",
        "Uses a semantic embedding (MiniLM) to capture contextual similarity beyond word frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c02b33b",
      "metadata": {
        "id": "7c02b33b"
      },
      "outputs": [],
      "source": [
        "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "X_train_emb = embedder.encode(X_train.tolist(), show_progress_bar=True, batch_size=128)\n",
        "X_test_emb = embedder.encode(X_test.tolist(), show_progress_bar=True, batch_size=128)\n",
        "\n",
        "embed_clf = LogisticRegression(max_iter=500, random_state=RANDOM_STATE)\n",
        "embed_clf.fit(X_train_emb, y_train)\n",
        "embed_preds = embed_clf.predict(X_test_emb)\n",
        "capture_metrics('MiniLM Embeddings + LogisticRegression', y_test, embed_preds)\n",
        "print(classification_report(y_test, embed_preds))\n",
        "plot_confusion(y_test, embed_preds, 'MiniLM Logistic Regression Confusion Matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7603df",
      "metadata": {
        "id": "6c7603df"
      },
      "source": [
        "## Configure HuggingFace for LLM-based Zero/Few-shot Classification\n",
        "You need a free HuggingFace API token from https://huggingface.co/settings/tokens. In Colab you can store it via `Tools → Secrets` and retrieve it with `google.colab.userdata.get('HF_API_KEY')`. Alternatively, set `os.environ['HF_API_KEY']` manually (just avoid hard-coding secrets in plain text).\n",
        "\n",
        "**Available Free Models:**\n",
        "- `meta-llama/Llama-3.2-3B-Instruct` - Fast and efficient for classification\n",
        "- `microsoft/Phi-3-mini-4k-instruct` - Compact model, good for simple tasks\n",
        "- `mistralai/Mistral-7B-Instruct-v0.3` - Balanced performance\n",
        "- `HuggingFaceH4/zephyr-7b-beta` - Good for instruction following\n",
        "\n",
        "**Rate Limits:** Free tier allows ~1,000 requests/day. We'll use a smaller sample size (100-200 messages) to stay within limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2d92beb",
      "metadata": {
        "id": "c2d92beb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "HF_API_KEY = os.environ.get('LLM_TOKEN')\n",
        "if HF_API_KEY is None:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        HF_API_KEY = userdata.get('LLM_TOKEN')\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "if not HF_API_KEY:\n",
        "    raise ValueError('Missing HuggingFace API key. Set HF_API_KEY via Colab secrets or environment variables before continuing.')\n",
        "\n",
        "# Using Phi-3-mini for fast inference and good performance on classification tasks\n",
        "HF_MODEL = 'microsoft/Phi-3-mini-4k-instruct'\n",
        "client = InferenceClient(token=HF_API_KEY)\n",
        "print(f'HuggingFace Inference API ready with model: {HF_MODEL}')\n",
        "print(f'Note: Free tier limit is ~1,000 requests/day. Adjust LLM_SAMPLE_SIZE accordingly.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b29d1e5",
      "metadata": {},
      "source": [
        "### Alternative HuggingFace Models for Spam Detection\n",
        "\n",
        "You can try different models by changing the `HF_MODEL` variable. Here are recommended free-tier options:\n",
        "\n",
        "**Recommended Models:**\n",
        "1. **`microsoft/Phi-3-mini-4k-instruct`** (Default)\n",
        "   - Size: 3.8B parameters\n",
        "   - Speed: Fast (~1-2s per request)\n",
        "   - Best for: Quick classification tasks\n",
        "   - Context: 4k tokens\n",
        "\n",
        "2. **`mistralai/Mistral-7B-Instruct-v0.3`**\n",
        "   - Size: 7B parameters\n",
        "   - Speed: Moderate (~2-3s per request)\n",
        "   - Best for: Better accuracy on nuanced messages\n",
        "   - Context: 8k tokens\n",
        "\n",
        "3. **`HuggingFaceH4/zephyr-7b-beta`**\n",
        "   - Size: 7B parameters\n",
        "   - Speed: Moderate (~2-3s per request)\n",
        "   - Best for: Instruction following\n",
        "   - Context: 8k tokens\n",
        "\n",
        "4. **`meta-llama/Llama-3.2-3B-Instruct`**\n",
        "   - Size: 3B parameters\n",
        "   - Speed: Very fast (~1s per request)\n",
        "   - Best for: Quick responses, good quality\n",
        "   - Context: 8k tokens\n",
        "\n",
        "**Free Tier Limits:**\n",
        "- **Rate Limit**: ~1,000 requests per day\n",
        "- **Token Limit**: Varies by model (typically 1,024-4,096 tokens per request)\n",
        "- **Concurrent Requests**: 1-2 at a time\n",
        "- Monitor usage at: https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c62ff66",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Test your HuggingFace API connection and estimate usage\n",
        "def test_hf_connection():\n",
        "    \"\"\"Test the HuggingFace API and estimate daily quota usage.\"\"\"\n",
        "    try:\n",
        "        test_message = \"Win a free iPhone now! Click here!\"\n",
        "        print(\"Testing HuggingFace API connection...\")\n",
        "        result = classify_with_hf(test_message)\n",
        "        print(f\"✓ API connection successful!\")\n",
        "        print(f\"✓ Test classification: '{test_message}' -> {result}\")\n",
        "        print(f\"\\nDaily quota estimate:\")\n",
        "        print(f\"  - LLM_SAMPLE_SIZE: {LLM_SAMPLE_SIZE} messages\")\n",
        "        print(f\"  - Estimated API calls: {LLM_SAMPLE_SIZE}\")\n",
        "        print(f\"  - Free tier limit: ~1,000 requests/day\")\n",
        "        print(f\"  - Remaining quota: ~{1000 - LLM_SAMPLE_SIZE} requests\")\n",
        "        print(f\"  - Estimated time: {(LLM_SAMPLE_SIZE * REQUEST_DELAY / 60):.1f} minutes\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ API connection failed: {e}\")\n",
        "        print(\"\\nTroubleshooting:\")\n",
        "        print(\"  1. Check your HF_API_KEY is valid\")\n",
        "        print(\"  2. Verify at https://huggingface.co/settings/tokens\")\n",
        "        print(\"  3. Ensure the token has 'Read' permissions\")\n",
        "        print(\"  4. Check if you've hit daily rate limits\")\n",
        "        return False\n",
        "\n",
        "# Uncomment to test your connection:\n",
        "# test_hf_connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e7af2d",
      "metadata": {
        "id": "07e7af2d"
      },
      "source": [
        "## LLM Inference Loop\n",
        "HuggingFace free tier has rate limits (~1,000 requests/day), so we evaluate on a smaller stratified subset of the held-out test set (default 100 messages). Adjust `LLM_SAMPLE_SIZE` based on your daily quota needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1efc3b67",
      "metadata": {
        "id": "1efc3b67"
      },
      "outputs": [],
      "source": [
        "# Reduced sample size to respect free tier limits (1,000 requests/day)\n",
        "LLM_SAMPLE_SIZE = 100  # Adjust based on your remaining daily quota\n",
        "llm_eval_df = (\n",
        "    pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
        "    .rename(columns={'message': 'text', 'label': 'label'})\n",
        ")\n",
        "if LLM_SAMPLE_SIZE and LLM_SAMPLE_SIZE < len(llm_eval_df):\n",
        "    llm_eval_df = (\n",
        "        llm_eval_df\n",
        "        .groupby('label', group_keys=False)\n",
        "        .apply(lambda grp: grp.sample(\n",
        "            n=max(1, int(LLM_SAMPLE_SIZE * len(grp) / len(llm_eval_df))),\n",
        "            random_state=RANDOM_STATE\n",
        "        ), include_groups=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are a strict SMS spam filter. Respond with ONLY one word: \"\n",
        "    \"either 'Spam' for unsolicited or fraudulent messages, or 'Ham' for regular \"\n",
        "    \"personal/business messages. Do not explain or add any other text.\"\n",
        ")\n",
        "\n",
        "def classify_with_hf(text: str, retry: int = 3, backoff: float = 3.0) -> str:\n",
        "    \"\"\"\n",
        "    Classify SMS message using HuggingFace Inference API.\n",
        "    Uses text_generation method which is more widely supported.\n",
        "    \"\"\"\n",
        "    # Combine system prompt and user message for text generation models\n",
        "    full_prompt = f\"{system_prompt}\\n\\nMessage: \\\"{text}\\\"\\nClassification:\"\n",
        "    \n",
        "    for attempt in range(retry):\n",
        "        try:\n",
        "            # Use text_generation instead of chat_completion for better compatibility\n",
        "            response = client.text_generation(\n",
        "                prompt=full_prompt,\n",
        "                model=HF_MODEL,\n",
        "                max_new_tokens=10,  # We only need one word\n",
        "                temperature=0.1,  # Low temperature for consistent classification\n",
        "                return_full_text=False\n",
        "            )\n",
        "            \n",
        "            raw = response.strip().lower()\n",
        "            \n",
        "            # Parse response - handle various formats\n",
        "            if 'spam' in raw and 'ham' in raw:\n",
        "                raw = raw.split()[0]\n",
        "            if 'spam' in raw:\n",
        "                return 'spam'\n",
        "            if 'ham' in raw:\n",
        "                return 'ham'\n",
        "                \n",
        "        except Exception as error:\n",
        "            error_str = str(error).lower()\n",
        "            # Handle rate limiting specifically\n",
        "            if 'rate limit' in error_str or '429' in error_str:\n",
        "                wait_time = backoff * (2 ** attempt)  # Exponential backoff\n",
        "                print(f'Rate limit hit. Waiting {wait_time:.1f}s before retry...')\n",
        "                time.sleep(wait_time)\n",
        "            elif attempt == retry - 1:\n",
        "                print(f'LLM classification failed after retries: {error}')\n",
        "                return 'ham'  # Default to ham on failure\n",
        "            else:\n",
        "                time.sleep(backoff * (attempt + 1))\n",
        "    \n",
        "    return 'ham'\n",
        "\n",
        "# Add small delay between requests to avoid rate limiting\n",
        "REQUEST_DELAY = 0.5  # seconds between requests\n",
        "\n",
        "llm_predictions = []\n",
        "print(f\"Processing {len(llm_eval_df)} messages with {REQUEST_DELAY}s delay between requests...\")\n",
        "print(f\"Estimated time: {(len(llm_eval_df) * REQUEST_DELAY / 60):.1f} minutes\\n\")\n",
        "\n",
        "for idx, row in llm_eval_df.iterrows():\n",
        "    prediction = classify_with_hf(row['text'])\n",
        "    llm_predictions.append(prediction)\n",
        "    \n",
        "    # Small delay to respect rate limits\n",
        "    if idx < len(llm_eval_df) - 1:\n",
        "        time.sleep(REQUEST_DELAY)\n",
        "    \n",
        "    if (idx + 1) % 10 == 0 or idx + 1 == len(llm_eval_df):\n",
        "        print(f\"Processed {idx + 1}/{len(llm_eval_df)} messages\")\n",
        "\n",
        "llm_eval_df['prediction'] = llm_predictions\n",
        "\n",
        "capture_metrics(f'{HF_MODEL} (LLM zero-shot)', llm_eval_df['label'], llm_eval_df['prediction'])\n",
        "print(classification_report(llm_eval_df['label'], llm_eval_df['prediction']))\n",
        "plot_confusion(llm_eval_df['label'], llm_eval_df['prediction'], f'{HF_MODEL.split(\"/\")[1]} Confusion Matrix (Sample)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a43b49c",
      "metadata": {
        "id": "0a43b49c"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "results_df.sort_values('f1', ascending=False).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b1cff4a",
      "metadata": {
        "id": "9b1cff4a"
      },
      "source": [
        "### Observations\n",
        "* **TF–IDF + Logistic Regression** mirrors the original Weka text-mining pipeline and usually delivers high recall on overt spam phrases such as \"free entry\" or \"claim now\".\n",
        "* **MiniLM embeddings** capture semantics and can reduce false positives on nuanced ham, at the cost of downloading the encoder and adding encoding latency.\n",
        "* **HuggingFace LLM** (Phi-3-mini) needs no training data but has rate limits on free tier (~1,000 requests/day); it performs well on context-heavy messages and can understand nuanced spam patterns.\n",
        "* **Rate Limit Management**: We use smaller sample sizes (100 messages) and add delays between requests to stay within free tier limits.\n",
        "* Hybrid scoring (e.g., fall back to HuggingFace LLM when the classical models disagree) is a strong extension for future lab work."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c0cfda",
      "metadata": {
        "id": "57c0cfda"
      },
      "source": [
        "## Optional: Export Artifacts\n",
        "If you want to retain the evaluation outputs in Drive, run the cell below and then use the Colab file browser or `drive.mount` to move the CSVs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "668b55b9",
      "metadata": {
        "id": "668b55b9"
      },
      "outputs": [],
      "source": [
        "results_df.to_csv('spam_lab_results_summary.csv', index=False)\n",
        "llm_eval_df.to_csv('spam_lab_llm_predictions.csv', index=False)\n",
        "print('Artifacts saved locally. Upload to Drive if you need persistent storage.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e098625",
      "metadata": {
        "id": "2e098625"
      },
      "source": [
        "## Next Steps\n",
        "1. Try different HuggingFace models like `mistralai/Mistral-7B-Instruct-v0.3` or `HuggingFaceH4/zephyr-7b-beta` to compare quality/speed trade-offs.\n",
        "2. Prompt-tune the LLM with few-shot examples in the system prompt for better performance on shorthand or code-mixed spam.\n",
        "3. Experiment with alternative embedding models (`all-mpnet-base-v2`, fastText) and blend their scores with the LLM for ensemble voting.\n",
        "4. Monitor your HuggingFace API usage at https://huggingface.co/settings/tokens to track daily limits.\n",
        "5. Consider upgrading to HuggingFace Pro ($9/month) for higher rate limits if you need to process more messages.\n",
        "6. Implement batching or caching strategies to minimize API calls while maximizing evaluation coverage."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
