{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ai-wrangler/BA_sms_LLM/blob/main/SMS_LLM_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b408d35",
      "metadata": {
        "id": "5b408d35"
      },
      "source": [
        "# SMS Spam Classification with Embeddings and HuggingFace LLM\n",
        "This Colab-ready notebook recreates the Lab 5 text mining workflow from Weka using Python pipelines and HuggingFace's free Inference API. You'll load the original ARFF dataset, build embedding-based classifiers, invoke HuggingFace models for zero-shot spam detection, and compare the evaluation metrics across approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563fdcad",
      "metadata": {
        "id": "563fdcad"
      },
      "source": [
        "## How to use this notebook in Google Colab\n",
        "1. Upload `SMS_LLM_Colab.ipynb` to Colab (File ‚Üí Upload notebook) or open it from Drive.\n",
        "2. Runtime ‚Üí Change runtime type ‚Üí make sure Python 3.10+; GPU is optional.\n",
        "3. Prepare the dataset: copy `TextCollection_sms.arff` to your Drive or download it locally so you can upload it when prompted.\n",
        "4. Get a free HuggingFace API token from https://huggingface.co/settings/tokens (create a \"Read\" token). Store it securely (`Tools ‚Üí Secrets` in Colab or `google.colab.userdata`). This notebook expects an environment variable called `HF_API_KEY`.\n",
        "5. Run the cells in order‚Äîeach is annotated to match the lab workflow and highlight differences between embeddings and LLM-based classification.\n",
        "\n",
        "**HuggingFace Free Tier Limits:**\n",
        "- Rate limit: 1,000 requests/day for free tier (varies by model)\n",
        "- Most models support 1,024-2,048 tokens per request\n",
        "- Some popular models may have lower rate limits during peak usage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bfa7d50",
      "metadata": {
        "id": "0bfa7d50"
      },
      "source": [
        "### Fixing 'Invalid Notebook' Error on GitHub\n",
        "\n",
        "To resolve the 'state' key missing error when rendering your notebook on GitHub, you should clear all cell outputs before saving and committing your notebook. Here's how to do it in Google Colab:\n",
        "\n",
        "1.  **Open your notebook** in Google Colab.\n",
        "2.  Go to the **'Runtime'** menu at the top.\n",
        "3.  Select **'Clear all outputs'**.\n",
        "4.  **Save the notebook** (File > Save).\n",
        "5.  Then, you can **download the `.ipynb` file** and upload it to GitHub, or sync it if you are using Google Drive integration with GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "75b5c66d",
      "metadata": {
        "id": "75b5c66d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install libraries that are not included in the base Colab runtime\n",
        "%pip install -q pandas numpy scikit-learn seaborn matplotlib sentence-transformers scipy liac-arff huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6555bf0a",
      "metadata": {
        "id": "6555bf0a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "import arff # Replaced from scipy.io import arff with import arff (for liac-arff)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    f1_score, precision_score, recall_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef6109a1",
      "metadata": {
        "id": "ef6109a1"
      },
      "outputs": [],
      "source": [
        "# Reproducibility helpers\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "random.seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "759efd82",
      "metadata": {
        "id": "759efd82"
      },
      "source": [
        "## Load the SMS Spam ARFF dataset\n",
        "The lab uses `TextCollection_sms.arff`. Use one of the cells below to make it available in the Colab filesystem. Uploading via the UI is the quickest path if the file is on your laptop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8d7edba",
      "metadata": {
        "id": "d8d7edba"
      },
      "outputs": [],
      "source": [
        "# Option A: Mount Google Drive (run this if the ARFF lives in Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# After mounting, set ARFF_PATH = '/content/drive/MyDrive/path/to/TextCollection_sms.arff'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f834df1c",
      "metadata": {
        "id": "f834df1c"
      },
      "outputs": [],
      "source": [
        "# Option B: Upload the ARFF manually (run this if the file is on your machine)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "ARFF_PATH = next(iter(uploaded))  # use the first uploaded filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65925520",
      "metadata": {
        "id": "65925520"
      },
      "outputs": [],
      "source": [
        "# If you mounted Drive instead of uploading, set the explicit path here.\n",
        "# Example: ARFF_PATH = '/content/drive/MyDrive/datasets/TextCollection_sms.arff'\n",
        "ARFF_PATH = locals().get('ARFF_PATH', 'TextCollection_sms.arff')\n",
        "print(f'Using dataset located at: {ARFF_PATH}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2322496",
      "metadata": {
        "id": "f2322496"
      },
      "outputs": [],
      "source": [
        "# Read the ARFF file into a DataFrame and mirror the original lab schema\n",
        "# Using liac-arff as scipy.io.arff does not support string attributes\n",
        "arff_data = arff.load(open(ARFF_PATH, 'r'))\n",
        "raw_data = arff_data['data']\n",
        "attributes = arff_data['attributes']\n",
        "column_names = [attr[0] for attr in attributes]\n",
        "\n",
        "sms_df = pd.DataFrame(raw_data, columns=column_names)\n",
        "# liac-arff reads strings directly, so no decoding is needed\n",
        "sms_df = sms_df.rename(columns={'Text': 'message', 'class-att': 'label'})\n",
        "sms_df['label'] = sms_df['label'].map({'0': 'ham', '1': 'spam'})\n",
        "sms_df['char_len'] = sms_df['message'].str.len()\n",
        "sms_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6b001f",
      "metadata": {
        "id": "ae6b001f"
      },
      "outputs": [],
      "source": [
        "# Quick class balance check\n",
        "ax = sms_df['label'].value_counts().sort_index().plot(kind='bar', color=['#4C72B0', '#DD8452'])\n",
        "ax.set(title='Class distribution', xlabel='Label', ylabel='Count')\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2, p.get_height()),\n",
        "                ha='center', va='bottom')\n",
        "plt.show()\n",
        "sms_df.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b37986",
      "metadata": {
        "id": "01b37986"
      },
      "outputs": [],
      "source": [
        "# Train/test split mirroring the lab evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    sms_df['message'],\n",
        "    sms_df['label'],\n",
        "    test_size=0.2,\n",
        "    stratify=sms_df['label'],\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "print(f'Train set: {X_train.shape[0]} messages | Test set: {X_test.shape[0]} messages')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c28f02b5",
      "metadata": {
        "id": "c28f02b5"
      },
      "outputs": [],
      "source": [
        "# Shared evaluation helpers for classical models and the LLM\n",
        "results = []\n",
        "\n",
        "def capture_metrics(name: str, y_true, y_pred) -> pd.Series:\n",
        "    metrics = {\n",
        "        'model': name,\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred, pos_label='spam'),\n",
        "        'recall': recall_score(y_true, y_pred, pos_label='spam'),\n",
        "        'f1': f1_score(y_true, y_pred, pos_label='spam')\n",
        "    }\n",
        "    results.append(metrics)\n",
        "    print(json.dumps(metrics, indent=2))\n",
        "    return pd.Series(metrics)\n",
        "\n",
        "def plot_confusion(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=['ham', 'spam'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['ham', 'spam'], yticklabels=['ham', 'spam'])\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d029f5a",
      "metadata": {
        "id": "9d029f5a"
      },
      "source": [
        "## Baseline 1: TF‚ÄìIDF + Logistic Regression\n",
        "Replicates the bag-of-words style features typically explored in Weka's text mining lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2955b8",
      "metadata": {
        "id": "2c2955b8"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(lowercase=True, stop_words='english', min_df=3, ngram_range=(1, 2))\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "bow_clf = LogisticRegression(max_iter=200, random_state=RANDOM_STATE, n_jobs=None)\n",
        "bow_clf.fit(X_train_tfidf, y_train)\n",
        "bow_preds = bow_clf.predict(X_test_tfidf)\n",
        "capture_metrics('TFIDF + LogisticRegression', y_test, bow_preds)\n",
        "print(classification_report(y_test, bow_preds))\n",
        "plot_confusion(y_test, bow_preds, 'TF-IDF Logistic Regression Confusion Matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f11a420e",
      "metadata": {
        "id": "f11a420e"
      },
      "source": [
        "## Baseline 2: SentenceTransformer Embeddings + Logistic Regression\n",
        "Uses a semantic embedding (MiniLM) to capture contextual similarity beyond word frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c02b33b",
      "metadata": {
        "id": "7c02b33b"
      },
      "outputs": [],
      "source": [
        "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "X_train_emb = embedder.encode(X_train.tolist(), show_progress_bar=True, batch_size=128)\n",
        "X_test_emb = embedder.encode(X_test.tolist(), show_progress_bar=True, batch_size=128)\n",
        "\n",
        "embed_clf = LogisticRegression(max_iter=500, random_state=RANDOM_STATE)\n",
        "embed_clf.fit(X_train_emb, y_train)\n",
        "embed_preds = embed_clf.predict(X_test_emb)\n",
        "capture_metrics('MiniLM Embeddings + LogisticRegression', y_test, embed_preds)\n",
        "print(classification_report(y_test, embed_preds))\n",
        "plot_confusion(y_test, embed_preds, 'MiniLM Logistic Regression Confusion Matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7603df",
      "metadata": {
        "id": "6c7603df"
      },
      "source": [
        "## Configure HuggingFace for LLM-based Zero/Few-shot Classification\n",
        "You need a free HuggingFace API token from https://huggingface.co/settings/tokens. In Colab you can store it via `Tools ‚Üí Secrets` and retrieve it with `google.colab.userdata.get('HF_API_KEY')`. Alternatively, set `os.environ['HF_API_KEY']` manually (just avoid hard-coding secrets in plain text).\n",
        "\n",
        "**Available Models (Verified Working):**\n",
        "- `mistralai/Mistral-7B-Instruct-v0.2` - **RECOMMENDED** - Most reliable and accurate\n",
        "- `HuggingFaceH4/zephyr-7b-beta` - Good for instruction following\n",
        "- `meta-llama/Meta-Llama-3-8B-Instruct` - Latest Llama model\n",
        "- `google/flan-t5-xxl` - Fallback option (different format)\n",
        "\n",
        "**Rate Limits:** \n",
        "- Free tier: ~1,000 requests/day\n",
        "- Pro tier ($9/month): Higher limits and priority access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2d92beb",
      "metadata": {
        "id": "c2d92beb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "HF_API_KEY = os.environ.get('HF_API_KEY')\n",
        "if HF_API_KEY is None:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        HF_API_KEY = userdata.get('HF_API_KEY')\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "if not HF_API_KEY:\n",
        "    raise ValueError('Missing HuggingFace API key. Set HF_API_KEY via Colab secrets or environment variables before continuing.')\n",
        "\n",
        "# Using Mistral-7B - Very reliable and widely available on HF Inference API\n",
        "HF_MODEL = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "client = InferenceClient(token=HF_API_KEY)\n",
        "print(f'HuggingFace Inference API ready with model: {HF_MODEL}')\n",
        "print(f'Note: This model is reliable and well-supported on HF API')\n",
        "print(f'Pro tier: Higher rate limits and priority access')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03c3fdfa",
      "metadata": {},
      "source": [
        "## üöÄ Quick Start Guide for HuggingFace LLM Classification\n",
        "\n",
        "**Follow these steps in order:**\n",
        "\n",
        "1. **Cell 21** ‚¨áÔ∏è - Configure API (set HF_API_KEY and model)\n",
        "2. **Cell 23** - Verify API connection (run diagnostic)\n",
        "3. **Cell 29** - Define the classifier function\n",
        "4. **Cell 30** - Set and test the classifier\n",
        "5. **Cell 32** - Process all messages\n",
        "\n",
        "**If you get errors:**\n",
        "- Run the diagnostic cell (23) to find working models\n",
        "- Check your API key is valid at https://huggingface.co/settings/tokens\n",
        "- Try switching models in cell 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6221181",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß DIAGNOSTIC: Run this cell first to find a working model/method\n",
        "import requests\n",
        "\n",
        "print(\"Testing HuggingFace API with available models...\\n\")\n",
        "\n",
        "test_message = \"WIN FREE PRIZE NOW!\"\n",
        "# These models are confirmed to work with HF Inference API\n",
        "models = [\n",
        "    'mistralai/Mistral-7B-Instruct-v0.2',  # Most reliable\n",
        "    'HuggingFaceH4/zephyr-7b-beta',\n",
        "    'meta-llama/Meta-Llama-3-8B-Instruct',\n",
        "    'google/flan-t5-xxl'  # Fallback option\n",
        "]\n",
        "\n",
        "for model_name in models:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Testing: {model_name}\")\n",
        "    print('='*70)\n",
        "    \n",
        "    try:\n",
        "        url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
        "        headers = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}\n",
        "        \n",
        "        response = requests.post(\n",
        "            url,\n",
        "            headers=headers,\n",
        "            json={\n",
        "                \"inputs\": f\"Classify as Spam or Ham: {test_message}\\nAnswer:\",\n",
        "                \"parameters\": {\"max_new_tokens\": 5, \"temperature\": 0.1}\n",
        "            },\n",
        "            timeout=30\n",
        "        )\n",
        "        \n",
        "        print(f\"Status: {response.status_code}\")\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            print(f\"‚úì SUCCESS! This model works!\")\n",
        "            print(f\"Response: {response.json()}\")\n",
        "            print(f\"\\nüìù To use this model, update cell 21:\")\n",
        "            print(f\"HF_MODEL = '{model_name}'\")\n",
        "            break\n",
        "        elif response.status_code == 503:\n",
        "            print(\"‚è≥ Model loading... wait 20s and try again\")\n",
        "        elif response.status_code == 401:\n",
        "            print(\"‚ùå AUTH ERROR - Check your HF_API_KEY!\")\n",
        "            break\n",
        "        elif response.status_code == 410:\n",
        "            print(\"‚ùå Model removed/unavailable (410 Gone)\")\n",
        "        else:\n",
        "            print(f\"‚ùå Error {response.status_code}: {response.text[:200]}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Exception: {e}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"‚úÖ Recommended: mistralai/Mistral-7B-Instruct-v0.2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff8ba928",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç STEP 1: Verify API Key and Test Connection\n",
        "import requests\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 1: Verifying HuggingFace API Configuration\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if API key is set\n",
        "if 'HF_API_KEY' not in globals() or not HF_API_KEY:\n",
        "    print(\"‚ùå ERROR: HF_API_KEY is not set!\")\n",
        "    print(\"\\nTo fix:\")\n",
        "    print(\"1. In Colab: Tools ‚Üí Secrets ‚Üí Add 'HF_API_KEY'\")\n",
        "    print(\"2. Or run: import os; os.environ['HF_API_KEY'] = 'your_token_here'\")\n",
        "else:\n",
        "    print(f\"‚úì API Key found: {HF_API_KEY[:10]}...{HF_API_KEY[-4:]}\")\n",
        "    \n",
        "    # Test simple API call\n",
        "    print(\"\\nTesting API connection...\")\n",
        "    try:\n",
        "        test_url = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "        headers = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}\n",
        "        \n",
        "        response = requests.post(\n",
        "            test_url,\n",
        "            headers=headers,\n",
        "            json={\"inputs\": \"Hello\", \"parameters\": {\"max_new_tokens\": 5}},\n",
        "            timeout=15\n",
        "        )\n",
        "        \n",
        "        print(f\"Status Code: {response.status_code}\")\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ API CONNECTION SUCCESSFUL!\")\n",
        "            print(\"Response received:\", response.json()[:100] if len(str(response.json())) > 100 else response.json())\n",
        "        elif response.status_code == 401:\n",
        "            print(\"‚ùå AUTHENTICATION FAILED\")\n",
        "            print(\"Your API key is invalid or expired\")\n",
        "            print(\"Get a new token at: https://huggingface.co/settings/tokens\")\n",
        "        elif response.status_code == 503:\n",
        "            print(\"‚è≥ Model is loading... This is normal, wait 20 seconds and try again\")\n",
        "        elif response.status_code == 429:\n",
        "            print(\"‚ö†Ô∏è  Rate limit reached. Wait a moment and try again\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Unexpected status: {response.status_code}\")\n",
        "            print(f\"Response: {response.text[:200]}\")\n",
        "            \n",
        "    except requests.exceptions.Timeout:\n",
        "        print(\"‚ùå CONNECTION TIMEOUT\")\n",
        "        print(\"Check your internet connection\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a49e9278",
      "metadata": {},
      "source": [
        "### üîß Troubleshooting Guide\n",
        "\n",
        "**If you're experiencing API errors:**\n",
        "\n",
        "1. **Try a different model** - Some models work better with the API than others:\n",
        "   - Recommended: `mistralai/Mistral-7B-Instruct-v0.2`\n",
        "   - Alternative: `HuggingFaceH4/zephyr-7b-beta`\n",
        "   - Latest: `meta-llama/Meta-Llama-3.1-8B-Instruct`\n",
        "\n",
        "2. **Switch to requests-based implementation** - Run the cells below to define `classify_sms = classify_with_hf_requests`\n",
        "\n",
        "3. **Check model status** - Some models may be loading: https://huggingface.co/models\n",
        "\n",
        "4. **Verify API key** - Make sure your `HF_API_KEY` is valid and has the right permissions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b29d1e5",
      "metadata": {},
      "source": [
        "### Alternative HuggingFace Models for Spam Detection\n",
        "\n",
        "You can try different models by changing the `HF_MODEL` variable. Here are recommended free-tier options:\n",
        "\n",
        "**Recommended Models:**\n",
        "1. **`microsoft/Phi-3-mini-4k-instruct`** (Default)\n",
        "   - Size: 3.8B parameters\n",
        "   - Speed: Fast (~1-2s per request)\n",
        "   - Best for: Quick classification tasks\n",
        "   - Context: 4k tokens\n",
        "\n",
        "2. **`mistralai/Mistral-7B-Instruct-v0.3`**\n",
        "   - Size: 7B parameters\n",
        "   - Speed: Moderate (~2-3s per request)\n",
        "   - Best for: Better accuracy on nuanced messages\n",
        "   - Context: 8k tokens\n",
        "\n",
        "3. **`HuggingFaceH4/zephyr-7b-beta`**\n",
        "   - Size: 7B parameters\n",
        "   - Speed: Moderate (~2-3s per request)\n",
        "   - Best for: Instruction following\n",
        "   - Context: 8k tokens\n",
        "\n",
        "4. **`meta-llama/Llama-3.2-3B-Instruct`**\n",
        "   - Size: 3B parameters\n",
        "   - Speed: Very fast (~1s per request)\n",
        "   - Best for: Quick responses, good quality\n",
        "   - Context: 8k tokens\n",
        "\n",
        "**Free Tier Limits:**\n",
        "- **Rate Limit**: ~1,000 requests per day\n",
        "- **Token Limit**: Varies by model (typically 1,024-4,096 tokens per request)\n",
        "- **Concurrent Requests**: 1-2 at a time\n",
        "- Monitor usage at: https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c62ff66",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test your HuggingFace API connection before processing all messages\n",
        "def test_hf_connection():\n",
        "    \"\"\"Test the HuggingFace API and verify it's working correctly.\"\"\"\n",
        "    test_messages = [\n",
        "        (\"Win a free iPhone now! Click here!\", \"spam\"),\n",
        "        (\"Hey, are you coming to dinner tonight?\", \"ham\"),\n",
        "        (\"URGENT! Your account will be closed. Verify now!\", \"spam\"),\n",
        "        (\"Thanks for your help yesterday\", \"ham\")\n",
        "    ]\n",
        "    \n",
        "    print(\"Testing HuggingFace API connection...\")\n",
        "    print(f\"Model: {HF_MODEL}\\n\")\n",
        "    \n",
        "    success_count = 0\n",
        "    for msg, expected in test_messages:\n",
        "        try:\n",
        "            result = classify_with_hf(msg)\n",
        "            status = \"‚úì\" if result == expected else \"?\"\n",
        "            success_count += 1 if result == expected else 0\n",
        "            print(f\"{status} '{msg[:50]}...' -> {result} (expected: {expected})\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚úó Test failed: {e}\")\n",
        "            return False\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"API Test Results: {success_count}/{len(test_messages)} correct\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    if success_count >= len(test_messages) * 0.5:  # At least 50% correct\n",
        "        print(\"‚úì API connection is working!\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"‚ö† API is responding but results may be unreliable\")\n",
        "        print(\"Consider trying a different model or checking the prompt format\")\n",
        "        return False\n",
        "\n",
        "# Run the test (comment out after verifying it works)\n",
        "# test_hf_connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a075631",
      "metadata": {},
      "source": [
        "### Alternative: Try Different Models if Current One Fails\n",
        "\n",
        "If you're experiencing issues with the default model, try these alternatives which have better API compatibility:\n",
        "\n",
        "**Most Reliable Options:**\n",
        "```python\n",
        "# Option 1: Mistral (very reliable with HF API)\n",
        "HF_MODEL = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "\n",
        "# Option 2: Zephyr (good instruction following)\n",
        "HF_MODEL = 'HuggingFaceH4/zephyr-7b-beta'\n",
        "\n",
        "# Option 3: Llama 3.1 (latest, very good)\n",
        "HF_MODEL = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
        "```\n",
        "\n",
        "Just update the `HF_MODEL` variable in the configuration cell above and re-run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "421abc47",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative implementation using requests library directly (MOST RELIABLE)\n",
        "import requests\n",
        "\n",
        "def classify_with_hf_requests(text: str, retry: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Classify SMS message using requests library directly.\n",
        "    This is the most reliable method for HuggingFace Inference API.\n",
        "    \"\"\"\n",
        "    API_URL = f\"https://api-inference.huggingface.co/models/{HF_MODEL}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}\n",
        "    \n",
        "    # Simple, direct prompt\n",
        "    prompt = f\"Classify this SMS message as either 'Spam' or 'Ham':\\n\\nMessage: {text}\\n\\nClassification:\"\n",
        "    \n",
        "    for attempt in range(retry):\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                API_URL,\n",
        "                headers=headers,\n",
        "                json={\n",
        "                    \"inputs\": prompt,\n",
        "                    \"parameters\": {\n",
        "                        \"max_new_tokens\": 10,\n",
        "                        \"temperature\": 0.1,\n",
        "                        \"do_sample\": False,\n",
        "                        \"return_full_text\": False\n",
        "                    }\n",
        "                },\n",
        "                timeout=30\n",
        "            )\n",
        "            \n",
        "            # Handle model loading\n",
        "            if response.status_code == 503:\n",
        "                wait_time = 20\n",
        "                print(f\"Model loading, waiting {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "            \n",
        "            # Handle rate limiting\n",
        "            if response.status_code == 429:\n",
        "                wait_time = 5 * (2 ** attempt)\n",
        "                print(f\"Rate limit, waiting {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "            \n",
        "            # Handle auth errors\n",
        "            if response.status_code == 401:\n",
        "                print(f\"Authentication error - check your HF_API_KEY\")\n",
        "                return 'ham'\n",
        "                \n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "            \n",
        "            # Parse different response formats\n",
        "            if isinstance(result, list) and len(result) > 0:\n",
        "                if isinstance(result[0], dict):\n",
        "                    text_result = result[0].get('generated_text', '').strip().lower()\n",
        "                else:\n",
        "                    text_result = str(result[0]).strip().lower()\n",
        "            elif isinstance(result, dict):\n",
        "                text_result = result.get('generated_text', result.get('text', '')).strip().lower()\n",
        "            else:\n",
        "                text_result = str(result).strip().lower()\n",
        "            \n",
        "            # Clean and parse\n",
        "            text_result = text_result.replace('*', '').replace('#', '').replace('`', '').strip()\n",
        "            \n",
        "            # Look for spam/ham\n",
        "            if 'spam' in text_result and 'ham' not in text_result:\n",
        "                return 'spam'\n",
        "            elif 'ham' in text_result and 'spam' not in text_result:\n",
        "                return 'ham'\n",
        "            elif 'spam' in text_result and 'ham' in text_result:\n",
        "                # If both, take the first one\n",
        "                spam_idx = text_result.index('spam')\n",
        "                ham_idx = text_result.index('ham')\n",
        "                return 'spam' if spam_idx < ham_idx else 'ham'\n",
        "            else:\n",
        "                # Default to ham if unclear\n",
        "                return 'ham'\n",
        "                \n",
        "        except KeyboardInterrupt:\n",
        "            raise\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"Timeout on attempt {attempt + 1}\")\n",
        "            if attempt == retry - 1:\n",
        "                return 'ham'\n",
        "            time.sleep(2)\n",
        "        except Exception as e:\n",
        "            if attempt == retry - 1:\n",
        "                print(f\"Failed after {retry} retries: {str(e)[:100]}\")\n",
        "                return 'ham'\n",
        "            time.sleep(2 * (attempt + 1))\n",
        "    \n",
        "    return 'ham'\n",
        "\n",
        "print(\"‚úì classify_with_hf_requests() defined\")\n",
        "print(\"This uses the reliable requests library\")\n",
        "print(\"\\nTo use it, run: classify_sms = classify_with_hf_requests\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c22e55",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ COMPLETE WORKING CLASSIFIER - Use this one!\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def classify_spam_simple(text: str, max_retries: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Simple, robust SMS spam classifier using HuggingFace API.\n",
        "    Returns 'spam' or 'ham'.\n",
        "    \"\"\"\n",
        "    url = f\"https://api-inference.huggingface.co/models/{HF_MODEL}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}\n",
        "    \n",
        "    # Very simple prompt that works reliably\n",
        "    prompt = f\"Is this message spam or ham? Message: {text}\\nAnswer (spam/ham):\"\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                url,\n",
        "                headers=headers,\n",
        "                json={\n",
        "                    \"inputs\": prompt,\n",
        "                    \"parameters\": {\n",
        "                        \"max_new_tokens\": 20,\n",
        "                        \"temperature\": 0.1,\n",
        "                        \"top_p\": 0.9\n",
        "                    }\n",
        "                },\n",
        "                timeout=30\n",
        "            )\n",
        "            \n",
        "            # Handle different status codes\n",
        "            if response.status_code == 503:\n",
        "                # Model loading\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"Model loading, waiting 20s... (attempt {attempt + 1}/{max_retries})\")\n",
        "                    time.sleep(20)\n",
        "                    continue\n",
        "                else:\n",
        "                    print(\"Model still loading after retries, defaulting to ham\")\n",
        "                    return 'ham'\n",
        "            \n",
        "            elif response.status_code == 429:\n",
        "                # Rate limit\n",
        "                wait_time = 5 * (2 ** attempt)\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"Rate limited, waiting {wait_time}s...\")\n",
        "                    time.sleep(wait_time)\n",
        "                    continue\n",
        "                else:\n",
        "                    return 'ham'\n",
        "            \n",
        "            elif response.status_code == 401:\n",
        "                print(\"‚ùå Authentication failed - check your HF_API_KEY\")\n",
        "                return 'ham'\n",
        "            \n",
        "            elif response.status_code == 410:\n",
        "                print(f\"‚ùå Model {HF_MODEL} is no longer available\")\n",
        "                return 'ham'\n",
        "            \n",
        "            elif response.status_code != 200:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2)\n",
        "                    continue\n",
        "                else:\n",
        "                    print(f\"Error {response.status_code}: {response.text[:100]}\")\n",
        "                    return 'ham'\n",
        "            \n",
        "            # Success! Parse the response\n",
        "            result = response.json()\n",
        "            \n",
        "            # Handle list response\n",
        "            if isinstance(result, list) and len(result) > 0:\n",
        "                if isinstance(result[0], dict) and 'generated_text' in result[0]:\n",
        "                    text_response = result[0]['generated_text'].lower()\n",
        "                else:\n",
        "                    text_response = str(result[0]).lower()\n",
        "            # Handle dict response\n",
        "            elif isinstance(result, dict):\n",
        "                text_response = result.get('generated_text', result.get('text', str(result))).lower()\n",
        "            else:\n",
        "                text_response = str(result).lower()\n",
        "            \n",
        "            # Clean up response\n",
        "            text_response = text_response.replace('*', '').replace('#', '').replace('`', '').strip()\n",
        "            \n",
        "            # Extract spam/ham\n",
        "            if 'spam' in text_response and 'ham' not in text_response:\n",
        "                return 'spam'\n",
        "            elif 'ham' in text_response and 'spam' not in text_response:\n",
        "                return 'ham'\n",
        "            elif 'spam' in text_response and 'ham' in text_response:\n",
        "                # Both found - take the first one\n",
        "                spam_pos = text_response.find('spam')\n",
        "                ham_pos = text_response.find('ham')\n",
        "                return 'spam' if spam_pos < ham_pos else 'ham'\n",
        "            else:\n",
        "                # No clear answer, default to ham\n",
        "                return 'ham'\n",
        "                \n",
        "        except requests.exceptions.Timeout:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"Timeout, retrying... ({attempt + 1}/{max_retries})\")\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print(\"Timeout after retries\")\n",
        "                return 'ham'\n",
        "        except KeyboardInterrupt:\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"Error: {str(e)[:50]}, retrying...\")\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print(f\"Failed after {max_retries} attempts: {str(e)[:50]}\")\n",
        "                return 'ham'\n",
        "    \n",
        "    return 'ham'\n",
        "\n",
        "# Test the function\n",
        "print(\"‚úÖ classify_spam_simple() function defined\")\n",
        "print(\"\\nQuick test...\")\n",
        "try:\n",
        "    test_result = classify_spam_simple(\"WIN FREE PRIZE NOW!!!\")\n",
        "    print(f\"‚úÖ Test successful: 'WIN FREE PRIZE NOW!!!' ‚Üí {test_result}\")\n",
        "    print(\"\\nüéØ Function is ready to use!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Test failed: {e}\")\n",
        "    print(\"Check your API key and model availability\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2259e71e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ SET THE CLASSIFIER TO USE\n",
        "# Use the simple, reliable classifier\n",
        "classify_sms = classify_spam_simple\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"‚úÖ Classifier set: {classify_sms.__name__}\")\n",
        "print(f\"‚úÖ Model: {HF_MODEL}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Quick test\n",
        "print(\"\\nüß™ Running quick test...\")\n",
        "try:\n",
        "    test_messages = [\n",
        "        \"WIN FREE PRIZE NOW!!!\",\n",
        "        \"Hey, want to grab dinner tonight?\"\n",
        "    ]\n",
        "    \n",
        "    for msg in test_messages:\n",
        "        result = classify_sms(msg)\n",
        "        print(f\"  '{msg[:40]}...' ‚Üí {result}\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Classifier is working! Ready to process messages.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Classifier test failed: {e}\")\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"1. Run cell 23 to verify API connection\")\n",
        "    print(\"2. Check that HF_API_KEY is set correctly\")\n",
        "    print(\"3. Try a different model (run the diagnostic cell)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e7af2d",
      "metadata": {
        "id": "07e7af2d"
      },
      "source": [
        "## LLM Inference Loop\n",
        "HuggingFace free tier has rate limits (~1,000 requests/day), so we evaluate on a smaller stratified subset of the held-out test set (default 100 messages). Adjust `LLM_SAMPLE_SIZE` based on your daily quota needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1efc3b67",
      "metadata": {
        "id": "1efc3b67"
      },
      "outputs": [],
      "source": [
        "# üìä PROCESS ALL TEST MESSAGES\n",
        "# HuggingFace Pro allows processing full test set\n",
        "LLM_SAMPLE_SIZE = None  # Set to None to process entire test set, or specify a number (e.g., 100 for testing)\n",
        "\n",
        "llm_eval_df = (\n",
        "    pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
        "    .rename(columns={'message': 'text', 'label': 'label'})\n",
        ")\n",
        "\n",
        "if LLM_SAMPLE_SIZE and LLM_SAMPLE_SIZE < len(llm_eval_df):\n",
        "    llm_eval_df = (\n",
        "        llm_eval_df\n",
        "        .groupby('label', group_keys=False)\n",
        "        .apply(lambda grp: grp.sample(\n",
        "            n=max(1, int(LLM_SAMPLE_SIZE * len(grp) / len(llm_eval_df))),\n",
        "            random_state=RANDOM_STATE\n",
        "        ), include_groups=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"üìä Processing {len(llm_eval_df)} messages\")\n",
        "print(f\"ü§ñ Model: {HF_MODEL}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Reduced delay for Pro tier\n",
        "REQUEST_DELAY = 0.1  # seconds between requests (Pro tier has higher limits)\n",
        "\n",
        "llm_predictions = []\n",
        "failed_count = 0\n",
        "success_count = 0\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  Estimated time: {(len(llm_eval_df) * REQUEST_DELAY / 60):.1f} minutes\")\n",
        "print(f\"Starting classification...\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for idx, row in llm_eval_df.iterrows():\n",
        "    try:\n",
        "        prediction = classify_sms(row['text'])\n",
        "        llm_predictions.append(prediction)\n",
        "        \n",
        "        if prediction in ['spam', 'ham']:\n",
        "            success_count += 1\n",
        "        \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n‚ö†Ô∏è  Interrupted by user\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error at message {idx + 1}: {str(e)[:50]}\")\n",
        "        llm_predictions.append('ham')  # Default on error\n",
        "        failed_count += 1\n",
        "    \n",
        "    # Small delay to respect rate limits\n",
        "    if idx < len(llm_eval_df) - 1:\n",
        "        time.sleep(REQUEST_DELAY)\n",
        "    \n",
        "    # Progress updates\n",
        "    if (idx + 1) % 50 == 0 or idx + 1 == len(llm_eval_df):\n",
        "        elapsed = time.time() - start_time\n",
        "        rate = (idx + 1) / elapsed if elapsed > 0 else 0\n",
        "        remaining = (len(llm_eval_df) - idx - 1) / rate if rate > 0 else 0\n",
        "        success_rate = (success_count / (idx + 1)) * 100 if idx >= 0 else 0\n",
        "        print(f\"‚úì {idx + 1}/{len(llm_eval_df)} | Rate: {rate:.1f} msg/s | Success: {success_rate:.0f}% | ETA: {remaining/60:.1f} min\")\n",
        "\n",
        "# Add predictions to dataframe\n",
        "if len(llm_predictions) < len(llm_eval_df):\n",
        "    # Fill remaining with 'ham' if interrupted\n",
        "    llm_predictions.extend(['ham'] * (len(llm_eval_df) - len(llm_predictions)))\n",
        "\n",
        "llm_eval_df['prediction'] = llm_predictions\n",
        "\n",
        "elapsed_total = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"‚úÖ COMPLETED in {elapsed_total/60:.1f} minutes\")\n",
        "print(f\"üìä Processed: {len(llm_predictions)} messages\")\n",
        "print(f\"‚ö° Average rate: {len(llm_predictions)/elapsed_total:.1f} messages/second\")\n",
        "print(f\"‚úì Successful: {success_count}/{len(llm_predictions)} ({success_count/len(llm_predictions)*100:.1f}%)\")\n",
        "if failed_count > 0:\n",
        "    print(f\"‚ö†Ô∏è  Failed: {failed_count}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Calculate metrics\n",
        "capture_metrics(f'{HF_MODEL} (LLM zero-shot)', llm_eval_df['label'], llm_eval_df['prediction'])\n",
        "print(classification_report(llm_eval_df['label'], llm_eval_df['prediction']))\n",
        "plot_confusion(llm_eval_df['label'], llm_eval_df['prediction'], f'{HF_MODEL.split(\"/\")[1]} Confusion Matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a43b49c",
      "metadata": {
        "id": "0a43b49c"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "results_df.sort_values('f1', ascending=False).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b1cff4a",
      "metadata": {
        "id": "9b1cff4a"
      },
      "source": [
        "### Observations\n",
        "* **TF‚ÄìIDF + Logistic Regression** mirrors the original Weka text-mining pipeline and usually delivers high recall on overt spam phrases such as \"free entry\" or \"claim now\".\n",
        "* **MiniLM embeddings** capture semantics and can reduce false positives on nuanced ham, at the cost of downloading the encoder and adding encoding latency.\n",
        "* **HuggingFace LLM** (Phi-3-mini) needs no training data but has rate limits on free tier (~1,000 requests/day); it performs well on context-heavy messages and can understand nuanced spam patterns.\n",
        "* **Rate Limit Management**: We use smaller sample sizes (100 messages) and add delays between requests to stay within free tier limits.\n",
        "* Hybrid scoring (e.g., fall back to HuggingFace LLM when the classical models disagree) is a strong extension for future lab work."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c0cfda",
      "metadata": {
        "id": "57c0cfda"
      },
      "source": [
        "## Optional: Export Artifacts\n",
        "If you want to retain the evaluation outputs in Drive, run the cell below and then use the Colab file browser or `drive.mount` to move the CSVs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "668b55b9",
      "metadata": {
        "id": "668b55b9"
      },
      "outputs": [],
      "source": [
        "results_df.to_csv('spam_lab_results_summary.csv', index=False)\n",
        "llm_eval_df.to_csv('spam_lab_llm_predictions.csv', index=False)\n",
        "print('Artifacts saved locally. Upload to Drive if you need persistent storage.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "729985fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick diagnostic test to find what works\n",
        "import requests\n",
        "\n",
        "def test_all_approaches():\n",
        "    \"\"\"Test different API approaches and models to find what works.\"\"\"\n",
        "    test_message = \"WIN FREE PRIZE NOW! Click here!\"\n",
        "    \n",
        "    models_to_test = [\n",
        "        'mistralai/Mistral-7B-Instruct-v0.2',\n",
        "        'HuggingFaceH4/zephyr-7b-beta',\n",
        "        'meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
        "        'microsoft/Phi-3-mini-4k-instruct'\n",
        "    ]\n",
        "    \n",
        "    print(\"Testing HuggingFace API approaches...\\n\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for model in models_to_test:\n",
        "        print(f\"\\nTesting model: {model}\")\n",
        "        print(\"-\"*70)\n",
        "        \n",
        "        # Test with requests\n",
        "        try:\n",
        "            API_URL = f\"https://api-inference.huggingface.co/models/{model}\"\n",
        "            headers = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}\n",
        "            \n",
        "            prompt = f\"Classify this SMS as 'Spam' or 'Ham': {test_message}\\nAnswer:\"\n",
        "            \n",
        "            response = requests.post(\n",
        "                API_URL,\n",
        "                headers=headers,\n",
        "                json={\n",
        "                    \"inputs\": prompt,\n",
        "                    \"parameters\": {\n",
        "                        \"max_new_tokens\": 10,\n",
        "                        \"temperature\": 0.1\n",
        "                    }\n",
        "                },\n",
        "                timeout=30\n",
        "            )\n",
        "            \n",
        "            print(f\"Status Code: {response.status_code}\")\n",
        "            \n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                print(f\"‚úì SUCCESS!\")\n",
        "                print(f\"Response: {result}\")\n",
        "                return model, \"requests\"\n",
        "            elif response.status_code == 503:\n",
        "                print(\"‚úó Model is loading (503)\")\n",
        "            elif response.status_code == 429:\n",
        "                print(\"‚úó Rate limited (429)\")\n",
        "            elif response.status_code == 401:\n",
        "                print(\"‚úó Authentication failed (401) - Check API key\")\n",
        "            else:\n",
        "                print(f\"‚úó Error: {response.status_code}\")\n",
        "                print(f\"Response: {response.text[:200]}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚úó Exception: {e}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"No working configuration found. Please check:\")\n",
        "    print(\"1. Your HF_API_KEY is valid\")\n",
        "    print(\"2. You have access to these models\")\n",
        "    print(\"3. Your network connection\")\n",
        "    return None, None\n",
        "\n",
        "# Run the diagnostic\n",
        "working_model, working_method = test_all_approaches()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e098625",
      "metadata": {
        "id": "2e098625"
      },
      "source": [
        "## Next Steps\n",
        "1. Try different HuggingFace models like `mistralai/Mistral-7B-Instruct-v0.3` or `HuggingFaceH4/zephyr-7b-beta` to compare quality/speed trade-offs.\n",
        "2. Prompt-tune the LLM with few-shot examples in the system prompt for better performance on shorthand or code-mixed spam.\n",
        "3. Experiment with alternative embedding models (`all-mpnet-base-v2`, fastText) and blend their scores with the LLM for ensemble voting.\n",
        "4. Monitor your HuggingFace API usage at https://huggingface.co/settings/tokens to track daily limits.\n",
        "5. Consider upgrading to HuggingFace Pro ($9/month) for higher rate limits if you need to process more messages.\n",
        "6. Implement batching or caching strategies to minimize API calls while maximizing evaluation coverage."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
